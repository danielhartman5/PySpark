{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9518128f597d7b00dc14729602cfd87fb7b2cf75925976bcb0d0e328a830a12b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Start Spark Session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WalmartStock\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"walmart_stock.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Look at data structure and top 5 rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "source": [
    "Basic stats of attributes, format numbers to 2 decimals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+--------+--------+--------+--------+-------------+\n|summary|    Open|    High|     Low|   Close|       Volume|\n+-------+--------+--------+--------+--------+-------------+\n|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258.00|\n|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093.50|\n| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,781.00|\n|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900.00|\n|    max|   90.80|   90.97|   89.25|   90.47|80,898,096.00|\n+-------+--------+--------+--------+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "describe_df = df.describe()\n",
    "\n",
    "describe_df.select(\"summary\",\n",
    "                    format_number(describe_df[\"Open\"].cast(\"float\"),2).alias(\"Open\"),\n",
    "                    format_number(describe_df[\"High\"].cast(\"float\"),2).alias(\"High\"),\n",
    "                    format_number(describe_df[\"Low\"].cast(\"float\"),2).alias(\"Low\"),\n",
    "                    format_number(describe_df[\"Close\"].cast(\"float\"),2).alias(\"Close\"),\n",
    "                    format_number(describe_df[\"Volume\"].cast(\"float\"),2).alias(\"Volume\"),\n",
    "                    ).show()"
   ]
  },
  {
   "source": [
    "Calculate HV ratio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------------------+\n|      Date|            HV Ratio|\n+----------+--------------------+\n|2012-01-03|4.819714653321546E-6|\n|2012-01-04|6.290848613094555E-6|\n|2012-01-05|4.669412994783916E-6|\n|2012-01-06|7.367338463826307E-6|\n|2012-01-09|8.915604778943901E-6|\n+----------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"HV Ratio\", df[\"High\"]/df[\"Volume\"])\n",
    "df2.select(\"Date\", \"HV Ratio\").show(5)"
   ]
  },
  {
   "source": [
    "Day of peak high"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2015-01-13'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df.orderBy(df[\"High\"].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+\n|       avg(Close)|\n+-----------------+\n|72.38844998012726|\n+-----------------+\n\n+-----------+-----------+\n|min(Volume)|max(Volume)|\n+-----------+-----------+\n|    2094900|   80898100|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Mean of Close\n",
    "# Min and Max of Volume\n",
    "\n",
    "from pyspark.sql.functions import mean, min, max\n",
    "df.select(mean(\"Close\")).show()\n",
    "\n",
    "df.select(min(\"Volume\"), max(\"Volume\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.141494435612083"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# Number of days close was lower than 60\n",
    "df.filter(df[\"Close\"] < 60).count()\n",
    "\n",
    "# Percentage of time the High was greater than 80 dollars\n",
    "cnt = df.filter(df[\"High\"] > 80).count()\n",
    "cnt/df.count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+\n| corr(High, Volume)|\n+-------------------+\n|-0.3384326061737161|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Pearson correlation between High and Volume\n",
    "from pyspark.sql.functions import corr\n",
    "df.select(corr(\"High\", \"Volume\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---------+\n|Year|max(High)|\n+----+---------+\n|2015|90.970001|\n|2013|81.370003|\n|2014|88.089996|\n|2012|77.599998|\n|2016|75.190002|\n+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# max high per year\n",
    "from pyspark.sql.functions import year\n",
    "year_df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "year_df.groupBy(\"Year\").max().select(\"Year\", \"max(High)\").show()"
   ]
  }
 ]
}