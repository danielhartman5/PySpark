{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9518128f597d7b00dc14729602cfd87fb7b2cf75925976bcb0d0e328a830a12b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"missingData\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"ContainsNull.csv\", inferSchema = True, header = True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Drop where there is null\n",
    "df.na.drop().show()\n",
    "\n",
    "# Drop when it has n number of null values\n",
    "df.na.drop(thresh=2).show() # needs to have at least 2 null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp2| null| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# drop only if all are null values\n",
    "df.na.drop(how=\"all\").show()\n",
    "\n",
    "\n",
    "# drop only if there is any null values\n",
    "df.na.drop(how=\"any\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# consider only on subset (where there is any nulls)\n",
    "df.na.drop(subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+-----+\n|  Id|   Name|Sales|\n+----+-------+-----+\n|emp1|   John| null|\n|emp2|No Data| null|\n|emp3|No Data|345.0|\n|emp4|  Cindy|456.0|\n+----+-------+-----+\n\n+----+-------+-----+\n|  Id|   Name|Sales|\n+----+-------+-----+\n|emp1|   John| null|\n|emp2|No Name| null|\n|emp3|No Name|345.0|\n|emp4|  Cindy|456.0|\n+----+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# filling missing data\n",
    "# can only be filled with the same data type, otherwise stays null\n",
    "df.na.fill(\"No Data\").show()\n",
    "\n",
    "df.na.fill(\"No Name\", subset=[\"Name\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Row(avg(Sales)=400.5)]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# fill missing sales with avg value\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "mean_val_row = df.select(mean(df[\"Sales\"])).collect()\n",
    "print(mean_val_row)\n",
    "# get just the value\n",
    "mean_val = mean_val_row[0][0]\n",
    "mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John|400.5|\n|emp2| null|400.5|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_val, [\"Sales\"]).show()"
   ]
  }
 ]
}